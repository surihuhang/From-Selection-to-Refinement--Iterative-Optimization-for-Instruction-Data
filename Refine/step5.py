## Repeat step3
import openai
import json
import concurrent.futures
import threading
import os
import time
import random

api_key = "fake-key1"

API_BASE = os.getenv("Strong_API_BASE", "")
MODEL_NAME = os.getenv("Strong_MODEL_NAME", "")
TEMPERATURE = 0.6
MAX_TOKENS = 4096
TIMEOUT = 15
MAX_RETRIES = 3

def chat_with_llama(api_key, user_prompt):
    openai.api_base = API_BASE
    openai.api_key = api_key

    system_prompt = '''
As an impartial evaluator, assess the quality of the following two instruction-tuning data samples.  
You should select the sample that provides the greatest benefit for model fine-tuning.  
Focus primarily on the **instruction** and **input** fields.  
The **output** field—generated by the same third-party LLM based on the instruction and input—may serve as a secondary reference.

Your evaluation should consider:  
- Correctness  
- Completeness  
- Clarity  
- Instruction adherence  
- Reasoning quality

First, compare the two samples and provide a brief explanation (no more than 100 words) to support potential improvements to the instruction and input.  
Avoid bias of any kind, and ensure the order of presentation does not affect your decision.  
Do not let the length of the answers influence your judgment.  
Remain as objective as possible.

After providing your explanation, output your final decision strictly in the following format:

Output format (JSON only):
{
  "winner": "A" | "B" | "Tie",
  "explanation": "Your short reasoning here. (≤100 words)"
}
    '''

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            resp = openai.ChatCompletion.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=TEMPERATURE,
                max_tokens=MAX_TOKENS,
                n=1,
                timeout=TIMEOUT,
            )
            return resp["choices"][0]["message"]["content"].strip()
        except Exception as e:
            err = f"ERROR: {str(e)}"
            # 退避一会儿再试
            if attempt < MAX_RETRIES:
                time.sleep(0.5 * attempt + random.random() * 0.5)
            else:
                return err

def step5_process_item(instruction, input, output_model, new_instruction, new_input, new_model_output):
    global counter
    instruction_old = instruction
    input_old = input
    output_model_old = output_model

    instruction_new = new_instruction
    input_new = new_input
    output_model_new = new_model_output

    Sample_A = {
        "instruction": instruction_old,
        "input": input_old,
        "output": output_model_old
    }
    Sample_B = {
        "instruction": instruction_new,
        "input": input_new,
        "output": output_model_new
    }

    user_prompt = f'''Now given:\n Sample A :{Sample_A}\n Sample B: {Sample_B}\n''' + '''\n{"winner": "A" | "B" | "Tie", "explanation": "Your short explanation here. (≤100 words)"}'''
    response = chat_with_llama(api_key, user_prompt)

    return response
